{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_ = pd.read_csv(\"training_v2.csv\")\r\n",
    "predict_ = pd.read_csv(\"unlabeled.csv\")\r\n",
    "\r\n",
    "train_['label'] = 'train'\r\n",
    "predict_['label'] = 'test'\r\n",
    "\r\n",
    "train_.drop(['encounter_id', 'hospital_id', 'patient_id', 'icu_id', 'readmission_status'], inplace=True, axis=1)\r\n",
    "predict_.drop(['encounter_id', 'hospital_id', 'patient_id', 'icu_id', 'readmission_status', \r\n",
    "'hospital_death'], inplace=True, axis=1)\r\n",
    "\r\n",
    "print(train_.shape)\r\n",
    "print(predict_.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(91713, 182)\n",
      "(39308, 181)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_[\"height\"] = np.where((train_.height.isna() & (train_.gender == 'F')), 160, train_[\"height\"])\r\n",
    "train_[\"height\"] = np.where((train_.height.isna() & (train_.gender == 'M')), 180, train_[\"height\"])\r\n",
    "train_[\"height\"] = np.where((train_.height.isna() & (train_.gender.isna())), 170, train_[\"height\"])\r\n",
    "train_[\"weight\"] = np.where((train_.height.isna() & (train_.gender == 'F')), 65, train_[\"weight\"])\r\n",
    "train_[\"weight\"] = np.where((train_.height.isna() & (train_.gender == 'M')), 82, train_[\"weight\"])\r\n",
    "train_[\"weight\"] = np.where((train_.height.isna() & (train_.gender.isna())), 74, train_[\"weight\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "predict_[\"height\"] = np.where((predict_.height.isna() & (predict_.gender == 'F')), 160, predict_[\"height\"])\r\n",
    "predict_[\"height\"] = np.where((predict_.height.isna() & (predict_.gender == 'M')), 180, predict_[\"height\"])\r\n",
    "predict_[\"height\"] = np.where((predict_.height.isna() & (predict_.gender.isna())), 170, predict_[\"height\"])\r\n",
    "predict_[\"weight\"] = np.where((predict_.height.isna() & (predict_.gender == 'F')), 65, predict_[\"weight\"])\r\n",
    "predict_[\"weight\"] = np.where((predict_.height.isna() & (predict_.gender == 'M')), 82, predict_[\"weight\"])\r\n",
    "predict_[\"weight\"] = np.where((predict_.height.isna() & (predict_.gender.isna())), 74, predict_[\"weight\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "lst = train_.isna().sum() / len(train_)\r\n",
    "\r\n",
    "p = pd.DataFrame(lst)\r\n",
    "\r\n",
    "# When we reset the index, the old index is added as a column, and a new sequential index is used\r\n",
    "p.reset_index(inplace=True)\r\n",
    "\r\n",
    "p.columns = ['a', 'b']\r\n",
    "low_count = p[p['b'] > 0.4]\r\n",
    "\r\n",
    "todelete = low_count['a'].values\r\n",
    "\r\n",
    "train_.drop(todelete, axis=1, inplace=True)\r\n",
    "predict_.drop(todelete, axis=1, inplace=True)\r\n",
    "\r\n",
    "train_.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(91713, 108)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_.dropna(thresh=54, inplace=True)\r\n",
    "train_.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(91580, 108)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_test = pd.concat([train_, predict_], keys=['x', 'y'])\r\n",
    "train_test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(130888, 108)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dictionary_ = pd.read_csv(\"WiDS Datathon 2020 Dictionary.csv\")\r\n",
    "print(dictionary_.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(188, 6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "integer_cols = []\r\n",
    "binary_cols = []\r\n",
    "numeric_cols = []\r\n",
    "string_cols = []\r\n",
    "\r\n",
    "for i in range(dictionary_.shape[0]):\r\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'integer':\r\n",
    "        integer_cols.append(dictionary_.loc[i, 'Variable Name'])\r\n",
    "\r\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'binary':\r\n",
    "        binary_cols.append(dictionary_.loc[i, 'Variable Name'])\r\n",
    "\r\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'numeric':\r\n",
    "        numeric_cols.append(dictionary_.loc[i, 'Variable Name'])\r\n",
    "\r\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'string':\r\n",
    "        string_cols.append(dictionary_.loc[i, 'Variable Name'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for col_name in numeric_cols:\r\n",
    "    if col_name in train_test.columns.to_list():\r\n",
    "        train_test[col_name] = train_test.groupby(['ethnicity', 'gender'], sort=False)[col_name].apply(lambda x: \r\n",
    "        x.fillna(x.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "\r\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\r\n",
    "train_test = imputer.fit_transform(train_test) \r\n",
    "\r\n",
    "train_test = pd.DataFrame(train_test, columns=train_.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "\r\n",
    "enc = OrdinalEncoder()\r\n",
    "\r\n",
    "for col_name in string_cols:\r\n",
    "    if col_name in train_test.columns.to_list():\r\n",
    "        train_test[col_name] = enc.fit_transform(train_test[[col_name]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_bmi_category(bmi):\r\n",
    "    if bmi != bmi:  # NaN\r\n",
    "        return np.nan\r\n",
    "    elif bmi < 18.5:  # Underweight\r\n",
    "        return 'Underweight'\r\n",
    "    elif bmi < 25:  # Healthy weight\r\n",
    "        return 'Healthy weight'\r\n",
    "    elif bmi < 30:  # Overweight\r\n",
    "        return 'Overweight'\r\n",
    "    else:  # Obese\r\n",
    "        return 'Obese'    \r\n",
    "    \r\n",
    "train_test[\"bmi_cat\"] = train_test[\"bmi\"].apply(get_bmi_category)\r\n",
    "\r\n",
    "# value_counts() function returns a Series containing counts of unique values. \r\n",
    "# The resulting object will be in descending order so that the first element is the most frequently-occurring element.\r\n",
    "train_test[\"bmi_cat\"] = train_test[\"bmi_cat\"].fillna(train_test[\"bmi_cat\"].value_counts().index[0])\r\n",
    "\r\n",
    "def get_blood_pressure_category(sysbp, diasbp):\r\n",
    "    if ((sysbp < 90) & (diasbp < 60)):\r\n",
    "        return 1  # Low blood pressure\r\n",
    "    elif ((sysbp < 120) & (diasbp < 80)):\r\n",
    "        return 2  # Normal\r\n",
    "    elif ((sysbp < 140) & (diasbp < 90)):\r\n",
    "        return 3  # Pre-Hypertension\r\n",
    "    elif ((sysbp < 160) & (diasbp < 100)):\r\n",
    "        return 4  # Stage 1 Hypertension\r\n",
    "    else:\r\n",
    "        return 5  # Stage 2 Hypertension\r\n",
    "\r\n",
    "train_test['bp_cat'] = train_test[['d1_sysbp_max', 'd1_diasbp_max']].apply(\r\n",
    "    lambda x: get_blood_pressure_category(x.d1_sysbp_max, x.d1_diasbp_max), axis=1)\r\n",
    "\r\n",
    "train_test[\"bp_cat\"] = train_test[\"bp_cat\"].fillna(train_test[\"bp_cat\"].value_counts().index[0])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# separate train and test\r\n",
    "train = train_test[train_test['label'] == \"train\"]\r\n",
    "predict = train_test[train_test['label'] == 'test']\r\n",
    "\r\n",
    "train.reset_index(inplace=True)\r\n",
    "train.drop(['label'], inplace=True, axis=1)\r\n",
    "\r\n",
    "predict.reset_index(inplace=True)\r\n",
    "predict.drop(['label'], inplace=True, axis=1)\r\n",
    "\r\n",
    "train = train.astype('float64')\r\n",
    "predict = predict.astype('float64')\r\n",
    "\r\n",
    "print(train.shape)\r\n",
    "print(predict.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\siddi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4901: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(91580, 108)\n",
      "(39308, 108)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "num_feature = []\r\n",
    "\r\n",
    "for col_name in numeric_cols:\r\n",
    "    if col_name in train_test.columns.to_list():\r\n",
    "        num_feature.append(col_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Create correlation matrix\r\n",
    "corr_matrix = train[num_feature].corr().abs()\r\n",
    "\r\n",
    "# Select upper triangle of correlation matrix\r\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\r\n",
    "\r\n",
    "# Find index of feature columns with correlation greater than 0.8\r\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\r\n",
    "\r\n",
    "train.drop(to_drop, inplace=True, axis=1)\r\n",
    "predict.drop(to_drop, inplace=True, axis=1)\r\n",
    "\r\n",
    "print(train.shape)\r\n",
    "print(predict.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(91580, 71)\n",
      "(39308, 71)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-15-aa439621f46a>:5: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "pca = PCA(n_components=2)\r\n",
    "components = pca.fit_transform(train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "\r\n",
    "fig = px.scatter(components, x=0, y=1, color=train['hospital_death'], width=1000, height=1000)\r\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import IsolationForest\r\n",
    "\r\n",
    "iso = IsolationForest(contamination=0.5)\r\n",
    "yhat = iso.fit_predict(train)\r\n",
    "\r\n",
    "# select all rows that are not outliers\r\n",
    "mask = yhat != -1\r\n",
    "len(mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.covariance import EllipticEnvelope\r\n",
    "\r\n",
    "ee = EllipticEnvelope(contamination=0.5)\r\n",
    "yhat = ee.fit_predict(train)\r\n",
    "\r\n",
    "mask = yhat != -1\r\n",
    "len(mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train['hospital_death'].value_counts() / len(train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    0.913791\n",
       "1.0    0.086209\n",
       "Name: hospital_death, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "y_train = train['hospital_death']\r\n",
    "\r\n",
    "train.drop(['hospital_death'], inplace=True, axis=1)\r\n",
    "\r\n",
    "cat_feature = []\r\n",
    "\r\n",
    "for i, col_name in enumerate(train.columns.to_list()):\r\n",
    "    if col_name not in numeric_cols:\r\n",
    "        cat_feature.append(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from imblearn.over_sampling import SMOTENC\r\n",
    "\r\n",
    "sm = SMOTENC(categorical_features=cat_feature)\r\n",
    "train_res, y_res = sm.fit_resample(train, y_train)\r\n",
    "\r\n",
    "y_train.value_counts() / len(train)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 359. GiB for an array with shape (75790, 5, 127025) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-afdc65ce96fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     81\u001b[0m         )\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mX_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_continuous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ohe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mX_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;31m# reverse the encoding of the categorical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             X_new, y_new = self._make_samples(\n\u001b[0m\u001b[0;32m    311\u001b[0m                 \u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\u001b[0m in \u001b[0;36m_make_samples\u001b[1;34m(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_num\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0my_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\u001b[0m in \u001b[0;36m_generate_samples\u001b[1;34m(self, X, nn_data, nn_num, rows, cols, steps)\u001b[0m\n\u001b[0;32m    577\u001b[0m             ] = self._X_categorical_minority_encoded\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mall_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnn_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         categories_size = [self.continuous_features_.size] + [\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 359. GiB for an array with shape (75790, 5, 127025) and data type float64"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "6d47ee5640fc503c71f4bb20d34a99a4db90cdbb2bf7627af55566e1cfb8cabe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
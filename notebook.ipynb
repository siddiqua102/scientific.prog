{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages and libraries needed to run the code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91713, 186)\n",
      "(39308, 186)\n"
     ]
    }
   ],
   "source": [
    "# import the training set and the unlabeled test set  \n",
    "X_train = pd.read_csv(\"training_v2.csv\")\n",
    "unlabeled = pd.read_csv(\"unlabeled.csv\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(unlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91713, 182)\n",
      "(39308, 181)\n"
     ]
    }
   ],
   "source": [
    "# drop features that are not expected to be related to the outcome\n",
    "X_train.drop(['hospital_id', 'patient_id', 'icu_id', 'readmission_status'], inplace=True, axis=1)\n",
    "unlabeled.drop(['hospital_id', 'patient_id', 'icu_id', 'readmission_status', 'hospital_death'], \n",
    "inplace=True, axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(unlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 6)\n"
     ]
    }
   ],
   "source": [
    "# read the dictionary \n",
    "dictionary_ = pd.read_csv(\"WiDS Datathon 2020 Dictionary.csv\")\n",
    "print(dictionary_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate lists of column names based on the data type of the columns\n",
    "integer_cols = []\n",
    "binary_cols = []\n",
    "numeric_cols = []\n",
    "string_cols = []\n",
    "\n",
    "for i in range(dictionary_.shape[0]):\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'integer':\n",
    "        integer_cols.append(dictionary_.loc[i, 'Variable Name'])\n",
    "\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'binary':\n",
    "        binary_cols.append(dictionary_.loc[i, 'Variable Name'])\n",
    "\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'numeric':\n",
    "        numeric_cols.append(dictionary_.loc[i, 'Variable Name'])\n",
    "\n",
    "    if dictionary_.loc[i, 'Data Type'] == 'string':\n",
    "        string_cols.append(dictionary_.loc[i, 'Variable Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f56bd5ced59f>:27: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "integer_cols_X_train = []\n",
    "binary_cols_X_train = []\n",
    "numeric_cols_X_train = []\n",
    "string_cols_X_train = []\n",
    "\n",
    "for col_name in X_train.columns.to_list():\n",
    "    if col_name in integer_cols:\n",
    "        integer_cols_X_train.append(col_name)\n",
    "\n",
    "    if col_name in binary_cols:\n",
    "        binary_cols_X_train.append(col_name)    \n",
    "    \n",
    "    if col_name in numeric_cols:\n",
    "        numeric_cols_X_train.append(col_name)\n",
    "\n",
    "    if col_name in string_cols:\n",
    "        string_cols_X_train.append(col_name)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 7))\n",
    "\n",
    "ax.bar(['integer', 'binary', 'numeric', 'string'], [len(integer_cols_X_train), \n",
    "len(binary_cols_X_train), len(numeric_cols_X_train), len(string_cols_X_train)])\n",
    "plt.xlabel('Type of column', labelpad=10)\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Raw data')\n",
    "\n",
    "fig.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_variable = 'ethnicity'\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(10, 10))\n",
    "#fig.suptitle('Distribution of %s' %cat_variable, size=15)\n",
    "labels = list(X_train[cat_variable].dropna().unique())\n",
    "#explode = (0.01, 0.01) \n",
    "explode = (0.05, 0.05, 0.05, 0.05, 0.2, 0.3)\n",
    "sizes = X_train[cat_variable].value_counts()\n",
    "ax.pie(sizes, explode=explode, startangle=60, labels=labels, \n",
    "#autopct='%1.1f%%', pctdistance=0.6,\n",
    "textprops={'fontsize': 20}, labeldistance=1.05) \n",
    "ax.add_artist(plt.Circle((0,0), 0.4, fc='white'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "sns.kdeplot(X_train[X_train.gender == 'F'].age, label='Female', color='salmon', bw_adjust=1.2)\n",
    "sns.kdeplot(X_train[X_train.gender == 'M'].age, label='Male', color='dodgerblue', bw_adjust=1.2)\n",
    "ax.set_title('Age [years]', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute height and weight with average values assuming that males are taller and heavier than females\n",
    "X_train[\"height\"] = np.where((X_train.height.isna() & (X_train.gender == 'F')), 160, X_train[\"height\"])\n",
    "X_train[\"height\"] = np.where((X_train.height.isna() & (X_train.gender == 'M')), 180, X_train[\"height\"])\n",
    "X_train[\"height\"] = np.where((X_train.height.isna() & (X_train.gender.isna())), 170, X_train[\"height\"])\n",
    "X_train[\"weight\"] = np.where((X_train.height.isna() & (X_train.gender == 'F')), 65, X_train[\"weight\"])\n",
    "X_train[\"weight\"] = np.where((X_train.height.isna() & (X_train.gender == 'M')), 82, X_train[\"weight\"])\n",
    "X_train[\"weight\"] = np.where((X_train.height.isna() & (X_train.gender.isna())), 74, X_train[\"weight\"])\n",
    "\n",
    "unlabeled[\"height\"] = np.where((unlabeled.height.isna() & (unlabeled.gender == 'F')), 160, unlabeled[\"height\"])\n",
    "unlabeled[\"height\"] = np.where((unlabeled.height.isna() & (unlabeled.gender == 'M')), 180, unlabeled[\"height\"])\n",
    "unlabeled[\"height\"] = np.where((unlabeled.height.isna() & (unlabeled.gender.isna())), 170, unlabeled[\"height\"])\n",
    "unlabeled[\"weight\"] = np.where((unlabeled.height.isna() & (unlabeled.gender == 'F')), 65, unlabeled[\"weight\"])\n",
    "unlabeled[\"weight\"] = np.where((unlabeled.height.isna() & (unlabeled.gender == 'M')), 82, unlabeled[\"weight\"])\n",
    "unlabeled[\"weight\"] = np.where((unlabeled.height.isna() & (unlabeled.gender.isna())), 74, unlabeled[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the distribution of height and weight in the data is approximately normal\n",
    "# Plot univariate or bivariate distributions using kernel density estimation.\n",
    "# A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations \n",
    "# in a dataset, analagous to a histogram. KDE represents the data using a continuous probability density \n",
    "# curve in one or more dimensions.\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "sns.kdeplot(X_train[X_train.gender == 'F'].weight, label='Female', color='salmon', ax=ax[0], bw_adjust=1.2)\n",
    "sns.kdeplot(X_train[X_train.gender == 'M'].weight, label='Male', color='dodgerblue', ax=ax[0], bw_adjust=1.2)\n",
    "ax[0].set_title('Weight [kg]', fontsize=14)\n",
    "\n",
    "sns.kdeplot(X_train[X_train.gender == 'F'].height, label='Female', color='salmon', ax=ax[1], bw_adjust=1.8)\n",
    "sns.kdeplot(X_train[X_train.gender == 'M'].height, label='Male', color='dodgerblue', ax=ax[1], bw_adjust=2)\n",
    "ax[1].set_title('Height [cm]', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91713, 108)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of missing values in each column and divide by column length\n",
    "lst = X_train.isna().sum() / len(X_train)\n",
    "\n",
    "# create a dataframe containing the percentage of missing values in each column\n",
    "p = pd.DataFrame(lst)\n",
    "\n",
    "# When we reset the index, the old index is added as a column, and a new sequential index is used\n",
    "p.reset_index(inplace=True)\n",
    "\n",
    "# find columns containing more than 30% missing values\n",
    "p.columns = ['a', 'b']\n",
    "low_count = p[p['b'] > 0.3]\n",
    "todelete = low_count['a'].values\n",
    "\n",
    "# drop columns containing more than 30% missing values from the training and unlabeled datasets\n",
    "X_train.drop(todelete, axis=1, inplace=True)\n",
    "unlabeled.drop(todelete, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-fab9be8b4b58>:28: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "integer_cols_X_train = []\n",
    "binary_cols_X_train = []\n",
    "numeric_cols_X_train = []\n",
    "string_cols_X_train = []\n",
    "\n",
    "for col_name in X_train.columns.to_list():\n",
    "    if col_name in integer_cols:\n",
    "        integer_cols_X_train.append(col_name)\n",
    "\n",
    "    if col_name in binary_cols:\n",
    "        binary_cols_X_train.append(col_name)    \n",
    "    \n",
    "    if col_name in numeric_cols:\n",
    "        numeric_cols_X_train.append(col_name)\n",
    "\n",
    "    if col_name in string_cols:\n",
    "        string_cols_X_train.append(col_name)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 7))\n",
    "\n",
    "ax.bar(['integer', 'binary', 'numeric', 'string'], [len(integer_cols_X_train), \n",
    "len(binary_cols_X_train), len(numeric_cols_X_train), len(string_cols_X_train)])\n",
    "plt.ylim(0, 160)\n",
    "plt.xlabel('Type of column', labelpad=10)\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Raw data')\n",
    "\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91679, 108)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows ontaining more than 30% missing values from the training dataset\n",
    "X_train.dropna(thresh=X_train.shape[1]*0.3, inplace=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the column containing labels from the training dataset and store it separately as a vector\n",
    "y_train = X_train['hospital_death']\n",
    "X_train.drop('hospital_death', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for string columns, replace empty boxes with \"None\"\n",
    "for col_name in string_cols:\n",
    "    if col_name in X_train.columns.to_list():\n",
    "        X_train[col_name].where(pd.notnull(X_train[col_name]), None, inplace=True)\n",
    "\n",
    "    if col_name in unlabeled.columns.to_list():\n",
    "        unlabeled[col_name].where(pd.notnull(unlabeled[col_name]), None, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode string columns as integers\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "\n",
    "for col_name in string_cols:\n",
    "    if col_name in X_train.columns.to_list():\n",
    "        X_train[col_name] = enc.fit_transform(X_train[[col_name]])\n",
    "\n",
    "    if col_name in unlabeled.columns.to_list():\n",
    "        unlabeled[col_name] = enc.fit_transform(unlabeled[[col_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91679, 107)\n",
      "(39308, 107)\n"
     ]
    }
   ],
   "source": [
    "# convert the data types of all columns to float \n",
    "X_train = X_train.astype('float32')\n",
    "unlabeled = unlabeled.astype('float32')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(unlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNImputer(copy=False, weights='distance')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute missing values using k nearest neighbors where k=5\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(missing_values=np.nan, n_neighbors=5, weights='distance', \n",
    "metric='nan_euclidean', copy=False)\n",
    "\n",
    "knn_imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0000000e+00, 5.6000000e+01, 2.1102440e+01, ..., 0.0000000e+00,\n",
       "        7.0000000e+00, 4.0000000e+00],\n",
       "       [5.0000000e+00, 6.1099613e+01, 1.9500595e+01, ..., 0.0000000e+00,\n",
       "        7.0000000e+00, 4.0000000e+00],\n",
       "       [7.0000000e+00, 6.6000000e+01, 3.2518597e+01, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       ...,\n",
       "       [1.3103900e+05, 7.1000000e+01, 2.3010380e+01, ..., 1.0000000e+00,\n",
       "        9.0000000e+00, 0.0000000e+00],\n",
       "       [1.3104100e+05, 5.7000000e+01, 3.2987949e+01, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [1.3105000e+05, 6.6000000e+01, 2.3183392e+01, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after fitting the imputer on the training set only, transform the training and unlabeled sets\n",
    "knn_imputer.transform(X_train)\n",
    "knn_imputer.transform(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data as csv files\n",
    "X_train.to_pickle(\"X_train_knn.csv\")\n",
    "unlabeled.to_pickle(\"unlabeled_1_knn.csv\")\n",
    "y_train.to_pickle(\"y_train_knn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the processed data from csv files\n",
    "X_train = pd.read_pickle(\"X_train_knn.csv\")\n",
    "y_train = pd.read_pickle(\"y_train_knn.csv\")\n",
    "unlabeled = pd.read_pickle(\"unlabeled_1_knn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of numeric columns names \n",
    "num_feature = []\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    if col_name in X_train.columns.to_list():\n",
    "        num_feature.append(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create correlation matrix of numeric columns \n",
    "corr_matrix = X_train[num_feature].corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap of the correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a graph of number of columns to drop vs correlation\n",
    "corr = []\n",
    "to_drop_columns = []\n",
    "\n",
    "for i in np.arange(0.5, 1, 0.1):\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > i)]\n",
    "    corr.append(i)\n",
    "    to_drop_columns.append(len(to_drop))\n",
    "\n",
    "plt.subplots(figsize=(10, 10))\n",
    "plt.plot(corr, to_drop_columns)\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of columns to drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index of feature columns with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "# drop columns that have more than 80% correlation to other columns\n",
    "X_train.drop(to_drop, inplace=True, axis=1)\n",
    "unlabeled.drop(to_drop, inplace=True, axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(unlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers using isolation forest algorithm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(n_estimators=1000, max_samples=1000, contamination='auto', \n",
    "max_features=10, bootstrap=False, n_jobs=-1, random_state=20)\n",
    "\n",
    "yhat = iso.fit_predict(X_train)\n",
    "\n",
    "scores = iso.decision_function(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "\n",
    "np.unique(mask, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_X_1, plot_X_2, plot_scores_1, plot_scores_2 = train_test_split(X_train, scores, \n",
    "test_size=0.10, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = plot_X_2.groupby('gender')\n",
    "for name, group in groups:\n",
    "    plt.plot(group.encounter_id, plot_scores_2, marker='o', linestyle='', markersize=12, label=name)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 15))\n",
    "plt.title(\"Isolation Forest\")\n",
    "\n",
    "b = plt.scatter(plot_X_2['encounter_id'], plot_scores_2, c=plot_X_2['age'], s=20)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.legend(handles=b.legend_elements()[0],  \n",
    "           title=\"age\")\n",
    "plt.xlabel('Encounter ID')\n",
    "plt.ylabel('Anomaly score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outlier samples from the training dataset\n",
    "X_train = X_train.loc[mask]\n",
    "y_train = y_train.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#num_feature = []\n",
    "\n",
    "#for col_name in numeric_cols:\n",
    "#    if col_name in X_train.columns.to_list():\n",
    "#        num_feature.append(col_name)\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "scalar.fit(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(scalar.transform(X_train), columns = X_train.columns)\n",
    "unlabeled = pd.DataFrame(scalar.transform(unlabeled), columns = unlabeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"X_train_scaled.csv\")\n",
    "unlabeled.to_pickle(\"unlabeled_1_scaled.csv\")\n",
    "y_train.to_pickle(\"y_train_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"X_train_scaled.csv\")\n",
    "y_train = pd.read_pickle(\"y_train_scaled.csv\")\n",
    "unlabeled = pd.read_pickle(\"unlabeled_1_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#X_train['hospital_death'] = y_train\n",
    "\n",
    "pca = PCA()\n",
    "components = pca.fit_transform(X_train)\n",
    "\n",
    "for i, var in enumerate(pca.explained_variance_ratio_ * 100):\n",
    "    print(f\"PC {i+1} ({var:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=y_train, width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(components, x=1, y=2, color=y_train, width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(components, x=0, y=2, color=X_train['ethnicity'], width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature = []\n",
    "\n",
    "for i, col_name in enumerate(X_train.columns.to_list()):\n",
    "    if col_name not in numeric_cols:\n",
    "        cat_feature.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train, y_train, test_size=0.5, stratify=y_train, \n",
    "random_state=20)\n",
    "X_train11, X_train21, y_train11, y_train21 = train_test_split(X_train1, y_train1, test_size=0.5, \n",
    "stratify=y_train1, random_state=20)\n",
    "X_train12, X_train22, y_train12, y_train22 = train_test_split(X_train2, y_train2, test_size=0.5, \n",
    "stratify=y_train2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "sm = SMOTENC(categorical_features=cat_feature, n_jobs=-1, random_state=20)\n",
    "\n",
    "X_train11, y_train11 = sm.fit_resample(X_train11, y_train11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train21, y_train21 = sm.fit_resample(X_train21, y_train21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train12, y_train12 = sm.fit_resample(X_train12, y_train12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train22, y_train22 = sm.fit_resample(X_train22, y_train22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train11.value_counts() / len(y_train11))\n",
    "print(y_train21.value_counts() / len(y_train21))\n",
    "print(y_train12.value_counts() / len(y_train12))\n",
    "print(y_train22.value_counts() / len(y_train22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train11, X_train21, X_train12, X_train22])\n",
    "y_train = pd.concat([y_train11, y_train21, y_train12, y_train22])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"X_train_smote.csv\")\n",
    "unlabeled.to_pickle(\"unlabeled_1_smote.csv\")\n",
    "y_train.to_pickle(\"y_train_smote.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"X_train_smote.csv\")\n",
    "y_train = pd.read_pickle(\"y_train_smote.csv\")\n",
    "unlabeled = pd.read_pickle(\"unlabeled_1_smote.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# make a pipeline to select features and apply logistic regression\n",
    "LR_pipeline = make_pipeline(SelectKBest(score_func=f_classif), LogisticRegression(solver='liblinear', \n",
    "random_state=20))\n",
    "\n",
    "# k = number of features to use in SelectKBest\n",
    "k_values = [20, 30, 40, 50, 60]\n",
    "\n",
    "# C = inverse regularization parameter for ridge or lasso; lowering C strengthens Lambda \n",
    "c_values = [100, 10, 1.0, 0.1]\n",
    "\n",
    "# l1 = lasso; l2 = ridge\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# define a grid of parameter values\n",
    "grid = dict(logisticregression__C=c_values, logisticregression__penalty=penalty, selectkbest__k=k_values)\n",
    "\n",
    "# define 5-fold cross-validation \n",
    "# KFold is a cross-validator that divides the dataset into k folds\n",
    "# Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label\n",
    "cv = StratifiedKFold(n_splits=5) \n",
    "\n",
    "# GridSearchCV exhaustively generates candidates from a grid of parameter values \n",
    "grid_search = GridSearchCV(estimator=LR_pipeline, param_grid=grid, n_jobs=-1, cv=cv)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV score: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "# define a logistic regression model using the optimal parameter values obtained from the grid search\n",
    "LR_pipeline = make_pipeline(SelectKBest(score_func=f_classif, k=grid_result.best_params_['selectkbest__k']), \n",
    "                        LogisticRegression(penalty=grid_result.best_params_['logisticregression__penalty'], \n",
    "                                           solver='liblinear', \n",
    "                                           C=grid_result.best_params_['logisticregression__C'], \n",
    "                                           random_state=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(0,len(grid_result.cv_results_['params']),8):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.bar(np.arange(len(grid_result.cv_results_['params'][i:i+8])),\n",
    "           grid_result.cv_results_['mean_test_score'][i:i+8],\n",
    "           yerr=grid_result.cv_results_['std_test_score'][i:i+8],\n",
    "           align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "    ax.set_ylabel('Mean')\n",
    "    ax.set_xticks(np.arange(len(grid_result.cv_results_['params'][i:i+8])))\n",
    "    ax.set_xticklabels(grid_result.cv_results_['params'][i:i+8], \n",
    "                       rotation=90)\n",
    "    ax.set_title('Cross-Validation Scores '+str(grid_result.cv_results_['params'][i])[0:30]+'}')\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    #plt.savefig(r'D:\\Maastricht\\Machine Learning\\Datasets\\Dataset1\\LogReg'+str(i)+'.png',\n",
    "               #dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# make a pipeline to select features and apply K Nearest Neighbors Classifier \n",
    "KNN_pipeline = make_pipeline(SelectKBest(score_func=f_classif), KNeighborsClassifier())\n",
    "\n",
    "# k = number of features to choose in SelectKBest\n",
    "k_values = [50]\n",
    "\n",
    "# number of neighbors to use for k neighbors queries\n",
    "#n_neighbors = range(100, 1000, 200)\n",
    "n_neighbors = [1000]\n",
    "\n",
    "# uniform: all points in each neighborhood are weighted equally.\n",
    "# distance: weight points by the inverse of their distance. Closer neighbors of a query point \n",
    "# will have a greater influence than neighbors which are further away.\n",
    "weights = ['uniform']\n",
    "\n",
    "# distance metric to measure nearset points\n",
    "metric = ['euclidean']\n",
    "\n",
    "# 5-fold cross-validation \n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "grid = dict(kneighborsclassifier__n_neighbors=n_neighbors, \n",
    "            kneighborsclassifier__weights=weights, \n",
    "            kneighborsclassifier__metric=metric, \n",
    "            selectkbest__k=k_values)\n",
    "\n",
    "# GridSearchCV exhaustively generates candidates from a grid of parameter values\n",
    "grid_search = GridSearchCV(estimator=KNN_pipeline, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', \n",
    "error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# define a K Nearest Neighbors model using the optimal parameter values obtained from the grid search\n",
    "KNN_pipeline = make_pipeline(SelectKBest(score_func=f_classif, k=grid_result.best_params_['selectkbest__k']), \n",
    "                        KNeighborsClassifier(metric=grid_result.best_params_['kneighborsclassifier__metric'], \n",
    "                                n_neighbors=grid_result.best_params_['kneighborsclassifier__n_neighbors'],\n",
    "                                             weights=grid_result.best_params_['kneighborsclassifier__weights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# make a pipeline to select features and apply Random Forest Classifier \n",
    "RF_pipeline = make_pipeline(SelectKBest(score_func=f_classif), RandomForestClassifier())\n",
    "\n",
    "# k = number of features to choose in SelectKBest\n",
    "k_values = [30, 40, 50, 60]\n",
    "\n",
    "# number of neighbors to use for k neighbors queries\n",
    "n_estimators = [1500]\n",
    "\n",
    "min_samples_split = [20, 50, 100]\n",
    "\n",
    "# 5-fold cross-validation \n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "grid = dict(randomforestclassifier__n_estimators=n_estimators, \n",
    "            randomforestclassifier__min_samples_split=min_samples_split,  \n",
    "            selectkbest__k=k_values)\n",
    "\n",
    "# GridSearchCV exhaustively generates candidates from a grid of parameter values\n",
    "grid_search = GridSearchCV(estimator=RF_pipeline, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', \n",
    "error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# define a Random Forest model using the optimal parameter values obtained from the grid search\n",
    "KNN_pipeline = make_pipeline(SelectKBest(score_func=f_classif, \n",
    "                                k=grid_result.best_params_['selectkbest__k']), \n",
    "                        RandomForestClassifier(\n",
    "                        n_estimators=grid_result.best_params_['randomforestclassifier__n_estimators'],\n",
    "                min_samples_split=grid_result.best_params_['randomforestclassifier__min_samples_split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=50, \n",
    "max_features='auto', bootstrap=True, n_jobs=-1, random_state=20)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 18))\n",
    "plt.barh(X_train.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = rf.predict(X_test)\n",
    "print(f\"accuracy score voting is {accuracy_score(y_test, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = rf.predict_proba(unlabeled)[:,1]\n",
    "\n",
    "submission = pd.DataFrame({'encounter_id': unlabeled['encounter_id'].astype(int), 'hospital_death': prob})\n",
    "\n",
    "print(len(prob))\n",
    "print(len(unlabeled['encounter_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d47ee5640fc503c71f4bb20d34a99a4db90cdbb2bf7627af55566e1cfb8cabe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
